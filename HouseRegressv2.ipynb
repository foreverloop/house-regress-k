{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#graphs and utilities\nimport os\nimport pandas as pd #pandas stands for panel data\nimport numpy as np\nimport math as ma\nimport seaborn as sns\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#Analysis\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom scipy.stats.stats import pearsonr\n\n\n#regression models\nfrom scipy.stats import linregress as linRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor\n\n#model metrics, deciding which models perform best\nfrom sklearn.metrics import mean_squared_error, r2_score\n\ndf_train = pd.read_csv(\"../input/train.csv\")\ndf_test = pd.read_csv(\"../input/test.csv\")\n\nall_dfs = [df_train,df_test]\n\ndef makeDict(df,label):\n    strcat_dict = {}\n    for i,row in df.iterrows():\n        strcat_dict[row[label]] = i \n    return strcat_dict\n\ndef makeOrdinal(df,label,showDict):\n    filtered = df.sort_values([label], ascending = [True])\n    df_filtered = filtered.groupby(label).first().reset_index()\n    strcat_dict = {}\n    \n    for i,row in df_filtered.iterrows():\n        strcat_dict[row[label]] = i \n    \n    if showDict:\n        print(strcat_dict)\n    \n    for j,row in df.iterrows():\n        df.at[j,label] = strcat_dict.get(row[label])\n    \n    return df\n\nprint(\"imports complete\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#quick way to see what data is missing \nsns.set_style(\"whitegrid\")\nmissing = df_train.isnull().sum()\nmissing = missing[missing > 0]\nmissing.sort_values(inplace=True)\nmissing.plot.bar()\n\n#90% threshold makes it ok to drop these columns, too much data missing to make these useful\ndrop_cols = []\nfor val,column in missing.iteritems():\n    if column > (len(df_train)*.90):\n        drop_cols.append(val)\n\nfor df in all_dfs:\n    df.drop(drop_cols,inplace=True, axis=1)\n\nall_dfs = [df_train,df_test]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"quantitative = [f for f in df_train.columns if df_train.dtypes[f] != 'object']\nquantitative.remove('SalePrice')\nquantitative.remove('Id')\nqualitative = [f for f in df_train.columns if df_train.dtypes[f] == 'object']\n\nfor column in qualitative:\n    for df_single in all_dfs:\n        makeOrdinal(df_single,column,False)\n        \nall_dfs = [df_train,df_test]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#distplot stands for distribution plot (so we can see how the data for SalePrice is distributed)\ny = df_train['SalePrice']\nplt.figure(1); plt.title('Johnson SU')\nsns.distplot(y, kde=False, fit=stats.johnsonsu)\nplt.figure(2); plt.title('Normal')\nsns.distplot(y, kde=False, fit=stats.norm)\nplt.figure(3); plt.title('Log Normal')\nsns.distplot(y, kde=False, fit=stats.lognorm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_normality = lambda x: stats.shapiro(x.fillna(0))[1] < 0.05\nnormal = pd.DataFrame(df_train[quantitative])\nnormal = normal.apply(test_normality)\nprint(not normal.any())\n#so none of the quantitiative variables are normally distrubted at a 1% significance level","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def spearman(frame, features):\n    spr = pd.DataFrame()\n    spr['feature'] = features\n    spr['spearman'] = [frame[f].corr(frame['SalePrice'], 'spearman') for f in features]\n    spr = spr.sort_values('spearman')\n    print(spr['feature'])\n    plt.figure(figsize=(6, 0.25*len(features)))\n    sns.barplot(data=spr, y='feature', x='spearman', orient='h')\n    \nspearman(df_train,quantitative)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spearman(df_train,qualitative)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bucketData(df,column_to_bucket):\n    number_of_buckets = 5\n    df['tempBand'] = pd.qcut(df[column_to_bucket],number_of_buckets)\n    intervals = df['tempBand'].unique().sort_values().tolist()\n    intervals = [x for x in intervals if str(x) != 'nan']\n    \n    interval_dict = {}\n    for idx,interval in enumerate(intervals):\n        interval_dict['low'+str(idx)] = interval.left\n        interval_dict['high'+str(idx)] = interval.right\n    \n    df.loc[ df[column_to_bucket] <= interval_dict['high0'], column_to_bucket] = 0\n    df.loc[(df[column_to_bucket] > interval_dict['low1']) & \\\n           (df[column_to_bucket] <= interval_dict['high1']), column_to_bucket] = 1\n    df.loc[(df[column_to_bucket] > interval_dict['low2']) & \\\n           (df[column_to_bucket] <= interval_dict['high2']), column_to_bucket] = 2\n    df.loc[(df[column_to_bucket] > interval_dict['low3']) & \\\n           (df[column_to_bucket] <= interval_dict['high3']), column_to_bucket] = 3\n    df.loc[ df[column_to_bucket] > interval_dict['low4'], column_to_bucket] = 4\n    \n    df = df.drop(['tempBand'], axis=1)\n\nfor df in all_dfs:\n    bucketData(df,'YearBuilt')\nfor df in all_dfs:\n    bucketData(df,'LotFrontage')\nfor df in all_dfs:\n    bucketData(df,'LotArea')\n\nall_dfs = [df_train,df_test]\nplt.hist(df_train['YearBuilt'])\nplt.hist(df_train['LotFrontage'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_imputer = SimpleImputer(strategy='mean')\n\nfloat_cols = [cname for cname in df_train.columns if df_train[cname].dtype in ['float64']]\nfloat_cols.extend(['KitchenQual','GarageFinish','BsmtQual','ExterQual','Foundation'])\n\nfor column in float_cols:\n    for df in all_dfs:\n        df[column] = df[column].fillna(int(df[column].mean()))\n\nsns.set_style(\"whitegrid\")\nmissing = df_train.isnull().sum()\nmissing = missing[missing > 0]\nmissing.sort_values(inplace=True)\nmissing.plot.bar()\n\ndf_train[float_cols] = df_train[float_cols].astype(int)\n\nhold_columns = df_train[float_cols].columns\ndf_train[float_cols] = pd.DataFrame(feature_imputer.fit_transform(df_train[float_cols].values))\ndf_train[float_cols].columns = hold_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_choices = ['YearRemodAdd','1stFlrSF','GarageYrBlt','TotalBsmtSF','FullBath','GarageArea',\\\n                 'LotFrontage','LotArea','YearBuilt','GarageCars','GrLivArea','OverallQual','SalePrice',\\\n                 'KitchenQual','GarageFinish','BsmtQual','ExterQual','Foundation']\n\nbest_features = df_train[final_choices]\n\ny = best_features['SalePrice']\nsale_predictors = best_features.drop(['SalePrice'], axis=1)\n\nX = sale_predictors\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,random_state=0)\n\nmy_imputer = SimpleImputer(strategy='mean')\nimputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\nimputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n\n# Imputation removed column names; put them back\nimputed_X_train.columns = X_train.columns\nimputed_X_valid.columns = X_valid.columns\n\nstdscale = StandardScaler()\nscaled_imputed_X_train = stdscale.fit_transform(imputed_X_train)\nscaled_imputed_X_valid = stdscale.fit_transform(imputed_X_valid)\n\npca_fin = scaled_imputed_X_train\npca_fin_val = scaled_imputed_X_valid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fit and predict with models and measure their accuracy\n#when r2 = 1, it's a perfect prediction\nreg_scores = pd.DataFrame(columns=['name','mean-sq-err','variance'])\nseed = 0\n\nlinear_reg = LinearRegression()\nlinear_reg.fit(pca_fin,y_train)\nlin_reg_pred = linear_reg.predict(pca_fin_val)\nreg_scores.loc[0] = ['linear regression',\\\n                     int(mean_squared_error(y_valid, lin_reg_pred)),r2_score(y_valid, lin_reg_pred)]\n\nada_reg = AdaBoostRegressor(DecisionTreeRegressor(max_depth=7),n_estimators=300, random_state=seed)\nada_reg.fit(pca_fin,y_train)\nada_reg_pred = ada_reg.predict(pca_fin_val)\nreg_scores.loc[1] = ['adaboost regression',\\\n                     int(mean_squared_error(y_valid, ada_reg_pred)),r2_score(y_valid, ada_reg_pred)]\n\nrf_reg = RandomForestRegressor(n_estimators=350, random_state=seed)\nrf_reg.fit(pca_fin,y_train)\nrf_reg_pred = rf_reg.predict(pca_fin_val)\nreg_scores.loc[2] = ['random forest regression',\\\n                     int(mean_squared_error(y_valid, rf_reg_pred)),r2_score(y_valid, rf_reg_pred)]\n\nknn_reg = KNeighborsRegressor(weights='uniform')\nknn_reg.fit(pca_fin,y_train)\nknn_reg_pred = knn_reg.predict(pca_fin_val)\nreg_scores.loc[3] = ['nearest neighbours regression',\\\n                     int(mean_squared_error(y_valid, knn_reg_pred)),r2_score(y_valid, knn_reg_pred)]\n\nbest_alpha = 0.0001\nregr = Lasso(alpha=best_alpha, max_iter=50000)\nregr.fit(pca_fin, y_train)\n\n# Run prediction on training set to get a rough idea of how well it does.\nlasso_reg_pred = regr.predict(pca_fin_val)\n\nreg_scores.loc[4] = ['lasso',\\\n                    int(mean_squared_error(y_valid, lasso_reg_pred)),r2_score(y_valid, lasso_reg_pred)]\n\n#only works on kaggle for now, since there's too many hoops to jump through to local install xgb\nxgb_reg = XGBRegressor()\nxgb_reg.fit(pca_fin, y_train)\nxgb_reg_pred = xgb_reg.predict(pca_fin_val)\nreg_scores.loc[5] = ['gradient boost regression',\\\n                     int(mean_squared_error(y_valid, xgb_reg_pred)),r2_score(y_valid, xgb_reg_pred)]\n\nreg_scores.head(6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_choices = ['YearRemodAdd','1stFlrSF','GarageYrBlt','TotalBsmtSF','FullBath','GarageArea',\\\n                 'LotFrontage','LotArea','YearBuilt','GarageCars','GrLivArea','OverallQual','SalePrice',\\\n                 'KitchenQual','GarageFinish','BsmtQual','ExterQual','Foundation']\n\nfinal_choices_test = ['YearRemodAdd','1stFlrSF','GarageYrBlt','TotalBsmtSF','FullBath','GarageArea',\\\n                 'LotFrontage','LotArea','YearBuilt','GarageCars','GrLivArea','OverallQual',\\\n                 'KitchenQual','GarageFinish','BsmtQual','ExterQual','Foundation']\n\nX_test = df_test[final_choices_test]\nbest_features = df_train[final_choices]\ny_train = best_features['SalePrice']\nsale_predictors = best_features.drop(['SalePrice'], axis=1)\nX_train = sale_predictors\n\nmy_imputer = SimpleImputer(strategy='mean')\nimputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train.values))\nimputed_X_valid = pd.DataFrame(my_imputer.transform(X_test.values))\n\nimputed_X_train.columns = X_train.columns\nimputed_X_valid.columns = X_test.columns\n\nstdscale = StandardScaler()\nscaled_imputed_X_train = stdscale.fit_transform(imputed_X_train)\nscaled_imputed_X_valid = stdscale.fit_transform(imputed_X_valid)\n\nxgb_reg = XGBRegressor()\nxgb_reg.fit(X_train, y_train)\nxgb_reg_pred = xgb_reg.predict(X_test)\n\nfinal_predictions = xgb_reg_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Submission = pd.DataFrame({ 'Id': df_test['Id'],\n                            'SalePrice': final_predictions })\n\nSubmission.to_csv(\"Submission3.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}